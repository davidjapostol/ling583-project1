{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "listed-alberta",
   "metadata": {},
   "source": [
    "# PROJECT 1: Categorizing news articles\n",
    "\n",
    "### Your task\n",
    "* Given a bunch of Reuters news service articles, develop a set of labels for categorizing them\n",
    "* Labels should be a single word or short phrase. Some articles might fit more than one label, and some might not fit any.\n",
    "* Aim for about 10–15 labels, give or take\n",
    "* Use methods from labs so far (keyword analysis, terminology extraction, topic models)\n",
    "* No specific ‘correct’ answer; the process you use to develop the list is more important than the solution.\n",
    "\n",
    "### Deliverables\n",
    "* List of labels\n",
    "* For each label, the number of articles from the dataset that fit that label\n",
    "* The number of articles that don't fit any of the labels (ideally this won't be a big number)\n",
    "* Annotated notebook showing your process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-threshold",
   "metadata": {},
   "source": [
    "Download packages for text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifteen-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import *\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import pycld2\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-anxiety",
   "metadata": {},
   "source": [
    "Grab the File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clinical-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('s3://ling583/project1.parquet', storage_options={'anon':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-chess",
   "metadata": {},
   "source": [
    "Set up Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prescribed-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', exclude=[\n",
    "                 'parser', 'ner', 'lemmatizer', 'attribute_ruler'])\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN', 'NNP']}},\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN',\n",
    "                                      'HYPH', 'NNP']}, 'OP': '*'},\n",
    "                      {'TAG': {'IN': ['NN', 'NNP']}}]])\n",
    "\n",
    "\n",
    "def get_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    spans = matcher(doc, as_spans=True)\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-tournament",
   "metadata": {},
   "source": [
    "Start a dask cluster to go through the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modern-sucking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37385</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.62 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37385' processes=4 threads=4, memory=16.62 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:37385\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-nowhere",
   "metadata": {},
   "source": [
    "Here we find domain-specific terminology that is relevant for Reuters Data by importing Spacy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turned-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "\n",
    "texts = dd.from_pandas(df['text'], npartitions=50).to_bag()\n",
    "\n",
    "graph = texts.map(get_candidates).flatten().frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comparative-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.59 s, sys: 797 ms, total: 6.39 s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "candidates = graph.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "passing-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import log2\n",
    "\n",
    "freqs = defaultdict(Counter)\n",
    "for c, f in candidates:\n",
    "    freqs[len(c)][c] += f\n",
    "\n",
    "\n",
    "def c_value(F, theta):\n",
    "\n",
    "    termhood = Counter()\n",
    "    longer = defaultdict(list)\n",
    "\n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "danish-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = c_value(freqs, theta=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-colonial",
   "metadata": {},
   "source": [
    "Save the MWE's to reuters-terms.txt for use in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "particular-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reuters-terms.txt', 'w') as f:\n",
    "    for t in terms:\n",
    "        print(' '.join(t), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-hampshire",
   "metadata": {},
   "source": [
    "Read the reuters data and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cardiovascular-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer(open('reuters-terms.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "supreme-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chicken-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "min_df = 100\n",
    "rm_top = 75\n",
    "tw = tp.TermWeight.ONE\n",
    "alpha = 0.1\n",
    "eta = 0.01\n",
    "tol = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-liquid",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "gentle-burke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c8a66540fe4e3ab52d51d6b42f724b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['tokens'] = pd.Series(df['text'].progress_apply(tokenizer.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-address",
   "metadata": {},
   "source": [
    "Estimate modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "still-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 LL = -7.8677\n",
      "   50 LL = -7.7306\n",
      "  100 LL = -7.6745\n",
      "  150 LL = -7.6443\n",
      "  200 LL = -7.6265\n",
      "  250 LL = -7.6128\n",
      "  300 LL = -7.6054\n",
      "  350 LL = -7.5990\n",
      "  400 LL = -7.5933\n",
      "  450 LL = -7.5907\n",
      "  500 LL = -7.5878\n",
      "  550 LL = -7.5854\n",
      "  600 LL = -7.5838\n",
      "  650 LL = -7.5830\n",
      "Done!\n",
      "CPU times: user 19min 23s, sys: 9.99 s, total: 19min 33s\n",
      "Wall time: 6min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mdl = tp.LDAModel(k=k, min_df=min_df, rm_top=rm_top, tw=tw, alpha=alpha, eta=eta)\n",
    "\n",
    "for doc in df['tokens']:\n",
    "    if doc:\n",
    "        mdl.add_doc(doc)\n",
    "\n",
    "last = np.NINF\n",
    "for i in range(0, 5000, 50):\n",
    "    mdl.train(50)\n",
    "    ll = mdl.ll_per_word\n",
    "    print(f'{i:5d} LL = {ll:7.4f}', flush=True)\n",
    "    if ll - last < tol:\n",
    "        break\n",
    "    else:\n",
    "        last = ll\n",
    "\n",
    "print(f'Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-jamaica",
   "metadata": {},
   "source": [
    "Apply topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "scheduled-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.DataFrame({'words': [' '.join(map(first, mdl.get_topic_words(k))) for k in range(mdl.k)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-surprise",
   "metadata": {},
   "source": [
    "Create topics.csv and add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "owned-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.to_csv('topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "revolutionary-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.read_csv('topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-sampling",
   "metadata": {},
   "source": [
    "For loop to count number of articles associated with each label as well as the number of articles NOT associated with ANY labels by subtracting the sum of articles associated from the total number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adequate-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotels': 2686, 'telecommunication': 2379, 'business': 10722, 'stock': 5297, 'airline': 4099, 'europe': 2138, 'labor': 4988, 'politics': 3587, 'asia': 4004, 'maritime transport': 3728, 'media': 1291, 'shipping': 2298, 'stocks': 1240, 'north america': 1276}\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "label_freqs = {}\n",
    "\n",
    "for article in mdl.docs:\n",
    "    for tag,prob in article.get_topics():\n",
    "        if prob > 0.01:\n",
    "            not_found = False\n",
    "            label = str(topics['label'].loc[tag])\n",
    "            if label == 'nan':\n",
    "                continue\n",
    "            found = label_freqs.get(label)\n",
    "            if found:\n",
    "                found = {label:found+1}\n",
    "            else:\n",
    "                found = {label:1}\n",
    "            label_freqs.update(found)\n",
    "print(label_freqs)\n",
    "print(50085 - sum(label_freqs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-equality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
